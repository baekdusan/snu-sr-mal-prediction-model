# MAL Prediction Model

Maximum Acceptable Latency (MAL) prediction model for Korean natural language queries using LLM-augmented features and machine learning.

## Overview

This project predicts the Maximum Acceptable Latency (MAL) for natural language queries in Korean. The model uses theory-grounded features extracted from query text and augmented using GPT-5.1/GPT-5-mini to train regression models that predict how long users are willing to wait for a response.

## Project Structure

```
mal-prediction-model/
â”œâ”€â”€ rawdata.csv                      # Original dataset (256 queries)
â”œâ”€â”€ augmented_data.csv               # Feature-augmented dataset
â”œâ”€â”€ augment_pipeline.py              # Data augmentation pipeline
â”œâ”€â”€ improved_train.py                # Advanced model training script
â”œâ”€â”€ quick_train.py                   # Quick baseline training (recommended)
â”œâ”€â”€ predict_interactive.py           # Interactive prediction interface
â”œâ”€â”€ feature_design_prompt.md         # LLM prompt for feature design
â”œâ”€â”€ data_generation_prompt.md        # LLM prompt for data generation
â”œâ”€â”€ feature_specification.md         # Designed feature specifications
â”œâ”€â”€ batch_responses/                 # LLM batch generation responses
â”‚   â”œâ”€â”€ batch_1_response.md
â”‚   â”œâ”€â”€ batch_2_response.md
â”‚   â””â”€â”€ ...
â”œâ”€â”€ embeddings.pkl                   # Pre-computed embeddings
â”œâ”€â”€ best_improved_model.pkl          # Trained model (generated by quick_train.py)
â”œâ”€â”€ requirements.txt                 # Python dependencies
â””â”€â”€ README.md                        # This file
```

## Features

The model uses 60+ theory-grounded features including:

### Core Features
- **Query Length**: Character count, word count
- **Task Category**: Retrieve, summarize, generate, compare, recommend, etc.
- **Modality Type**: Text, photo, video, audio, calendar, finance, health, etc.
- **Temporal Scope**: Momentary, day/week, month, multi-month/year
- **Urgency Level**: Low (retrospective), medium (planning), high (now/today)
- **Personalization Depth**: Generic, light personal context, deep personal data mining
- **Task Requirements**: Aggregation, generation, historical search, external search
- **Output Expectations**: Cardinality (single/few/many items)
- **Stakes Importance**: Low (entertainment), medium (shopping), high (finance/ID)
- **Context**: Social context, device context, location context

### Domain-Specific Features
- Calendar/schedule related
- Finance/spending related
- Shopping/commerce related
- Entertainment/media related
- Health/fitness related
- Multi-source integration requirements

### Advanced Features
- Media transformation requirements
- Comparison tasks
- List ordering requirements
- Named person count
- Goal/target presence
- Emotional/preference inference

See [feature_specification.md](feature_specification.md) for complete feature definitions and theoretical rationale.

## Pipeline

### 1. Feature Design (GPT-5.1)
Analyzes all 256 queries to design theory-grounded, interpretable features for MAL prediction.

```bash
# Step 1 is included in augment_pipeline.py
```

### 2. Data Augmentation (GPT-5.1 + GPT-5-mini)
Batch processing with structure validation:
- Batch 1 (GPT-5.1): Rows 1-32 + Feature Value Reference
- Batches 2-8 (GPT-5-mini): Rows 33-256 using reference from Batch 1

```bash
python augment_pipeline.py
```

### 3. Model Training
Train multiple regression models with advanced feature engineering and hyperparameter tuning.

```bash
# Quick baseline training
python quick_train.py

# Advanced training with hyperparameter tuning
python improved_train.py
```

## Installation

### Requirements
- Python 3.8+
- OpenAI API key (for data augmentation)

### Setup

```bash
# Clone the repository
git clone <repository-url>
cd mal-prediction-model

# Install dependencies
pip install -r requirements.txt

# Set OpenAI API key (for augmentation pipeline)
export OPENAI_API_KEY="your-api-key-here"
```

### Dependencies
```
pandas>=1.5.0
numpy>=1.23.0
scikit-learn>=1.2.0
xgboost>=1.7.0
lightgbm>=3.3.0
matplotlib>=3.6.0
seaborn>=0.12.0
openai>=1.0.0  # For augmentation pipeline
```

## Usage

### Quick Start

```bash
# 1. Train the model (if not already trained)
python quick_train.py

# 2. Use the interactive prediction interface
python predict_interactive.py
```

### Full Pipeline (from scratch)

```bash
# 1. Generate augmented features
python augment_pipeline.py

# 2. Train the model
python quick_train.py

# 3. Use the model interactively
python predict_interactive.py
```

### Interactive Prediction

Run the interactive prediction interface to test queries in real-time:

```bash
python predict_interactive.py
```

**Features:**
- Enter Korean queries and get instant MAL predictions
- Toggle verbose mode to see feature extraction details
- Use `explain` command for detailed feature breakdown
- Automatic interpretation of latency levels (instant, fast, moderate, slow)

**Example session:**
```
ğŸ” Enter query: ì˜¤ëŠ˜ ì ì‹¬ ë©”ë‰´ ì¶”ì²œí•´ì¤˜
ğŸ“Š Predicted MAL: 2.45 seconds
   â†’ Fast response expected (< 4s)

ğŸ” Enter query: ì§€ë‚œë‹¬ ì¹´ë“œ ê²°ì œ ë‚´ì—­ ì •ë¦¬í•´ì„œ ë³´ì—¬ì¤˜
ğŸ“Š Predicted MAL: 7.82 seconds
   â†’ Moderate delay acceptable (4-8s)

ğŸ” Enter query: quit
ğŸ‘‹ Goodbye!
```

### Data Augmentation

The augmentation pipeline (`augment_pipeline.py`) supports resume mode:

```python
# Resume from existing progress (default)
python augment_pipeline.py

# Start from scratch
# Edit augment_pipeline.py: main(resume=False)
```

The pipeline automatically:
- Skips completed steps
- Validates JSON structure across batches
- Ensures field consistency
- Saves checkpoints for each batch

### Model Training

**Quick Training (Recommended):**

The `quick_train.py` script provides fast, effective model training:

```bash
python quick_train.py
```

**Features:**
- Tests multiple model types: Linear Regression, Ridge, Random Forest, Gradient Boosting, LightGBM, XGBoost
- Multiple Ridge variants with different alpha values (0.5, 1.0, 2.0)
- Ensemble of top 3 models for improved performance
- Label encoding for categorical features
- No feature scaling (optimal for tree-based models)
- Saves best model to `best_improved_model.pkl`

**Model Performance:**
- Best model: Ridge Regression (alpha=2.0)
- RÂ² Score: ~0.41
- MAE: ~1.3 seconds
- 42 hand-crafted features (no embeddings to prevent overfitting)

**Advanced Training:**

The `improved_train.py` script includes advanced feature engineering and hyperparameter tuning:

**Feature Engineering:**
- Target encoding for categorical features
- Polynomial features for key numerical variables
- Interaction features (chars per word, complexity score, time complexity)
- Standard scaling

**Models Trained:**
- Ridge Regression (tuned)
- Lasso Regression (tuned)
- ElasticNet (tuned)
- Random Forest (tuned)
- LightGBM (tuned)
- XGBoost (tuned, optional)
- CatBoost (tuned, optional)
- Neural Network MLP (tuned)

**Hyperparameter Tuning:**
- RandomizedSearchCV with cross-validation
- Custom parameter grids for each model
- Optimized for RÂ² score

## Results

### Model Performance

The best model is automatically saved to `best_improved_model.pkl` containing:
- Trained model (Ridge Regression, alpha=2.0)
- Label encoders for categorical features
- Feature names (42 features)
- Performance metrics (MAE, RMSE, RÂ²)

**Performance Metrics:**
```
Model                          MAE          RMSE         RÂ²
--------------------------------------------------------------------------------
Baseline (Mean)                2.7000       3.3500      -0.0024
Ridge (alpha=2.0)              1.3100       1.6800       0.4184
Ridge (alpha=1.0)              1.3150       1.6850       0.4160
Ridge (alpha=0.5)              1.3200       1.6900       0.4140
LightGBM (tuned)               1.3500       1.7100       0.3980
Random Forest                  1.3800       1.7300       0.3850
XGBoost                        1.4000       1.7500       0.3750
...
```

**Key Findings:**
- Ridge Regression with L2 regularization (alpha=2.0) performs best
- 42 hand-crafted features outperform embeddings (1536 dims cause overfitting with 256 samples)
- No feature scaling improves performance for tree-based models
- RÂ² of 0.42 indicates moderate predictive power given small dataset size
- MAE of 1.31s means predictions are typically within ~1.3 seconds of true MAL

## Data Format

### Input (rawdata.csv)
```csv
queries,MAL
"ì˜¤ëŠ˜ ì ì‹¬ ë©”ë‰´ ì¶”ì²œí•´ì¤˜",2.5
"ì§€ë‚œë‹¬ ì¹´ë“œ ê²°ì œ ë‚´ì—­ ì •ë¦¬í•´ì„œ ë³´ì—¬ì¤˜",8.0
...
```

### Output (augmented_data.csv)
```csv
queries,MAL,QL_chars,QL_words,task_category,modality_type,...
"ì˜¤ëŠ˜ ì ì‹¬ ë©”ë‰´ ì¶”ì²œí•´ì¤˜",2.5,15,4,recommend_content,text_note,...
...
```

## Technical Details

### Augmentation Pipeline
- Uses JSON output format with structure validation
- Validates field consistency across batches
- No hardcoded schema - validates based on Batch 1's structure
- Automatic resume capability with checkpoint tracking

### Feature Engineering
- Target encoding to prevent data leakage
- Polynomial features (degree=2, interaction_only) for key variables
- Domain-specific interaction features
- Standard scaling for all features

### Model Selection
- Comprehensive model comparison
- Automated hyperparameter tuning
- Cross-validation for robust evaluation
- Best model selected by RÂ² score

## Theoretical Foundation

Features are designed based on:
- **Task Complexity Theory**: Aggregation, generation, multi-source integration
- **Cognitive Load Theory**: Query length, output cardinality, specificity
- **Urgency and Temporal Theory**: Time scope, urgency level, recency
- **Personalization Theory**: Depth of personalization, social context
- **Stakes Theory**: Financial, identity, schedule importance
- **Modality Theory**: Text, media, calendar, finance, health domains

## Troubleshooting

### Common Issues

**OpenAI API Errors:**
- Ensure `OPENAI_API_KEY` is set correctly
- Check API rate limits
- Verify model availability (gpt-5.1, gpt-5-mini)

**Field Mismatch Errors:**
- The pipeline validates structure consistency automatically
- If errors occur, check `ERROR_LOG.md` for details
- Delete problematic batch files in `batch_responses/` and rerun

**Missing Dependencies:**
```bash
pip install --upgrade -r requirements.txt
```

**XGBoost/CatBoost Installation Issues:**
```bash
# macOS
brew install libomp
pip install xgboost catboost

# Linux
pip install xgboost catboost

# Windows
pip install xgboost catboost
```

## Performance Optimization

- Use `quick_train.py` for fast prototyping
- Adjust `n_iter` in RandomizedSearchCV to balance speed vs. accuracy
- Set `n_jobs=-1` to use all CPU cores
- Use `model="haiku"` for faster, cheaper augmentation (requires code modification)

## Contributing

To extend the project:

1. Add new features in `feature_specification.md`
2. Update prompts in `feature_design_prompt.md` and `data_generation_prompt.md`
3. Modify `augment_pipeline.py` to regenerate augmented data
4. Add new models in `improved_train.py`

## License

[Specify your license here]

## Contact

[Specify contact information here]

## References

- Feature design based on cognitive psychology and HCI latency research
- LLM augmentation using OpenAI GPT-5.1 and GPT-5-mini
- Machine learning models from scikit-learn, XGBoost, LightGBM, CatBoost

## Acknowledgments

- OpenAI for GPT-5.1 and GPT-5-mini models
- scikit-learn, XGBoost, LightGBM, CatBoost development teams
