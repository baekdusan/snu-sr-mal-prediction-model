# Commercial MAL Predictor - Usage Guide

**Production-ready MAL prediction for any Korean query**

---

## ğŸ¯ What This Does

Predicts **Maximum Acceptable Latency (MAL)** for Korean queries at different user retention levels.

**Key Concept - Retention vs Churn**:
- **50% retention** (50% churn) = Median MAL â†’ half of users will leave
- **90% retention** (10% churn) = Higher MAL â†’ keep 90% of users, only 10% leave
- **95% retention** (5% churn) = Even higher MAL â†’ keep 95% of users, only 5% leave

**The higher the retention rate, the higher the MAL you need to design for.**

---

## ğŸ“¦ Installation

```bash
pip install openai pandas numpy scipy statsmodels
```

Set your OpenAI API key:
```bash
export OPENAI_API_KEY="sk-..."
```

---

## ğŸš€ Quick Start

### **Method 1: Interactive Mode** (ì§ì ‘ ì…ë ¥)

```bash
cd projects/LMM_model/scripts
python commercial_predictor_llm.py --interactive
```

Then type your queries:
```
Enter your Korean query: ì§€ë‚œì£¼ì— ì°ì€ ê³¨í”„ ìŠ¤ìœ™ ì˜ìƒ ë³´ì—¬ì¤˜

ğŸ” Analyzing: 'ì§€ë‚œì£¼ì— ì°ì€ ê³¨í”„ ìŠ¤ìœ™ ì˜ìƒ ë³´ì—¬ì¤˜'
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Extracting features: 'ì§€ë‚œì£¼ì— ì°ì€ ê³¨í”„ ìŠ¤ìœ™ ì˜ìƒ ë³´ì—¬ì¤˜...'
  âœ“ Features extracted

ğŸ“Š Extracted Features:
  âœ“ QL_media_domain

â±ï¸  Predicted MAL:
  â€¢ 50% retention (50% churn): 12.3s (keep 50% of users)
  â€¢ 90% retention (10% churn): 28.5s (keep 90% of users)
  â€¢ 95% retention (5% churn): 35.7s (keep 95% of users)

  Recommendation:
    â†’ Design for 28.5s to keep 90% of users (10% churn)

Mean MAL: 12.3s (50th percentile)
```

### **Method 2: Python API**

```python
from commercial_predictor_llm import CommercialMALPredictorLLM

# Initialize (one time)
predictor = CommercialMALPredictorLLM()

# Predict for a query
query = "ì˜¤ëŠ˜ ë‚ ì”¨ì— ì í•©í•œ íŒ¨ì…˜ ìŠ¤íƒ€ì¼ ì¶”ì²œí•´ì¤˜"
result = predictor.predict(query, accommodation_levels=[50, 90, 95])

print(result['mal_predictions'])
# Output: {'50%': 10.5, '90%': 24.3, '95%': 30.5}

print(result['interpretation'])
# Output:
#   â€¢ 50% retention (50% churn): 10.5s (keep 50% of users)
#   â€¢ 90% retention (10% churn): 24.3s (keep 90% of users)
#   â€¢ 95% retention (5% churn): 30.5s (keep 95% of users)
#
#   Recommendation:
#     â†’ Design for 24.3s to keep 90% of users (10% churn)
```

### **Method 3: Batch Processing**

```python
queries = [
    "ì§€ë‚œì£¼ì— ì°ì€ ê³¨í”„ ìŠ¤ìœ™ ì˜ìƒ ë³´ì—¬ì¤˜",
    "ì˜¤ëŠ˜ ë‚ ì”¨ì— ì í•©í•œ íŒ¨ì…˜ ìŠ¤íƒ€ì¼ ì¶”ì²œí•´ì¤˜",
    "ì´ë²ˆ ë‹¬ì— ì´ ì–¼ë§ˆë‚˜ ì¼ì§€?",
    "ì†Œì—°ì´ê°€ ì¶”ì²œí–ˆë˜ ì˜í™” ì½˜í…ì¸  ì œëª©ì´ ë­ì˜€ì§€?"
]

df_results = predictor.batch_predict(queries, accommodation_levels=[50, 90, 95])
df_results.to_csv('mal_predictions.csv', index=False)
```

---

## ğŸ“Š Understanding the Output

### **Retention Rate Interpretation**

| Retention | Churn | MAL Example | Meaning |
|-----------|-------|-------------|---------|
| **50%** | 50% | 12.3s | Half of users will leave if latency > 12.3s |
| **90%** | 10% | 28.5s | Only 10% will leave if latency â‰¤ 28.5s |
| **95%** | 5% | 35.7s | Only 5% will leave if latency â‰¤ 35.7s |

**Design Rule**:
- For **critical features**: Design for 95% retention (low churn tolerance)
- For **standard features**: Design for 90% retention
- For **experimental features**: Design for 50-75% retention

### **Feature Extraction**

The predictor automatically extracts 8 features using GPT-4o-mini:

1. `QL_requires_structured_output` - Needs list/table/formatted output
2. `QL_long_horizon_planning` - Long-term analysis (yearly, semi-annual)
3. `QL_calendar_schedule_domain` - Calendar, meetings, appointments
4. `QL_social_context` - Involves people, friends, groups
5. `QL_weather_coupled` - Weather-related
6. `QL_media_domain` - Music, movies, videos, podcasts
7. `QL_question_formality` - Polite question form vs imperative
8. `QL_recall_specific_entity` - "Where/when/what was it" queries

---

## ğŸ’¡ Example Use Cases

### **Use Case 1: Mobile App Design**

```python
# Test critical user flow
query = "í˜„ì¬ ìœ„ì¹˜ì—ì„œ ê±¸ì–´ì„œ ê°ˆ ìˆ˜ ìˆëŠ” ë§›ì§‘ ì¶”ì²œí•´ì¤˜"
result = predictor.predict(query, accommodation_levels=[90, 95])

print(f"Design latency target: {result['mal_predictions']['90%']}s")
# Design for 90% retention to minimize churn
```

### **Use Case 2: A/B Testing**

```python
# Test different query phrasings
queries = [
    "ìŒì•… ì¶”ì²œí•´ì¤˜",  # Short imperative
    "ìŒì•… ì¶”ì²œí•´ì¤„ë˜?",  # Polite question
    "ë‚˜í•œí…Œ ë§ëŠ” ìŒì•… ë¦¬ìŠ¤íŠ¸ ë§Œë“¤ì–´ì¤˜"  # Personalized + structured
]

for query in queries:
    result = predictor.predict(query, verbose=False)
    print(f"{query:40s} â†’ 90% retention: {result['mal_predictions']['90%']}s")
```

### **Use Case 3: SLA Definition**

```python
# Define SLAs based on query type
query_types = {
    "urgent": "ì§€ê¸ˆ ì—¬ê¸° í• ì¸ë˜ëŠ” ì¹´ë“œ ë­ì•¼?",
    "standard": "ì´ë²ˆ ë‹¬ì— ì´ ì–¼ë§ˆë‚˜ ì¼ì§€?",
    "analytical": "ì˜¬í•´ ìƒë°˜ê¸°ì— ê°€ì¥ ë§ì´ ì‚° ì¹´í…Œê³ ë¦¬ ë­ì•¼?"
}

for qtype, query in query_types.items():
    result = predictor.predict(query, verbose=False)
    print(f"{qtype:12s} SLA: {result['mal_predictions']['90%']:.1f}s (90% retention)")
```

---

## ğŸ”§ Advanced Usage

### **Custom Retention Levels**

```python
result = predictor.predict(
    "ì¿¼ë¦¬",
    accommodation_levels=[10, 25, 50, 75, 90, 95, 99]
)
```

### **Silent Mode (no prints)**

```python
result = predictor.predict(query, verbose=False)
```

### **Access Raw Features**

```python
result = predictor.predict(query)

# See which features were detected
for feat, val in result['features'].items():
    if val == 1:
        print(f"âœ“ {feat}")
```

---

## âš¡ Performance

- **Feature extraction**: ~0.5-1s per query (GPT-4o-mini)
- **Model prediction**: <1ms
- **Total**: ~0.5-1s per query
- **Accuracy**: RÂ² = 0.73 (73% variance explained)
- **Cost**: ~$0.0001 per query (GPT-4o-mini pricing)

---

## ğŸ“ Technical Details

### **Model**
- **Type**: Linear Mixed Model (LMM)
- **Features**: 8 (down from 51)
- **Random Effect**: Participant (accounts for individual differences)
- **Prediction**: Population-level (works for all users)

### **Assumptions**
- Log-normal distribution of MAL
- 70.5% of variance is due to individual differences
- 29.5% is due to query characteristics

### **Validation**
- Training data: 2,560 observations (80 participants Ã— 256 queries)
- RÂ² = 0.7277
- RMSE = 1.55 seconds
- All features: p < 0.001 (highly significant)
- No multicollinearity (VIF < 10)

---

## â“ FAQ

**Q: What if my query is in English?**
A: The model is trained on Korean queries only. English queries may not work well.

**Q: Can I predict for a specific user?**
A: Currently, this is population-level prediction (average across all users). Individual prediction would require user-specific data.

**Q: What if I don't have OpenAI API key?**
A: You need an API key for LLM feature extraction. Alternative: Use rule-based extraction (see `commercial_predictor_final.py`).

**Q: How accurate is this?**
A: RÂ² = 0.73 means the model explains 73% of variance. The remaining 27% is due to individual differences and noise.

**Q: Should I design for 90% or 95% retention?**
A:
- **90% (10% churn)**: Standard for most features
- **95% (5% churn)**: Critical features, low churn tolerance
- **50% (50% churn)**: Exploratory features, high churn acceptable

---

## ğŸ“ Files

- `commercial_predictor_llm.py`: Main predictor (LLM-based)
- `models/lmm_model1_selected.pkl`: Trained model (8 features)
- `docs/feature_specification_selected.md`: Feature definitions

---

## ğŸš€ Production Deployment

```python
# Example: Flask API
from flask import Flask, request, jsonify
from commercial_predictor_llm import CommercialMALPredictorLLM

app = Flask(__name__)
predictor = CommercialMALPredictorLLM()

@app.route('/predict', methods=['POST'])
def predict():
    data = request.json
    query = data.get('query')
    retention_levels = data.get('retention_levels', [50, 90, 95])

    result = predictor.predict(query, accommodation_levels=retention_levels, verbose=False)

    return jsonify({
        'query': query,
        'predictions': result['mal_predictions'],
        'mean_mal': result['mean_mal'],
        'interpretation': result['interpretation']
    })

if __name__ == '__main__':
    app.run(port=5000)
```

---

## ğŸ“ Support

For issues or questions, refer to:
- Feature definitions: [feature_specification_selected.md](../docs/feature_specification_selected.md)
- Model training: `retrain_model1_selected.py`
- Feature analysis: `feature_selection_analysis.py`
