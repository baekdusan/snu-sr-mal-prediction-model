# MAL (Maximum Acceptable Latency) Prediction Model

**ì„ í˜• í˜¼í•© ëª¨ë¸(Linear Mixed Model)ì„ ì´ìš©í•œ MAL ì˜ˆì¸¡**

---

## ğŸ“‹ ëª©ì°¨

1. [í”„ë¡œì íŠ¸ ê°œìš”](#í”„ë¡œì íŠ¸-ê°œìš”)
2. [ë””ë ‰í† ë¦¬ êµ¬ì¡°](#ë””ë ‰í† ë¦¬-êµ¬ì¡°)
3. [ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸](#ë°ì´í„°-ì²˜ë¦¬-íŒŒì´í”„ë¼ì¸)
4. [ëª¨ë¸ë§ ë°©ë²•ë¡ ](#ëª¨ë¸ë§-ë°©ë²•ë¡ )
5. [ëª¨ë¸ ì„±ëŠ¥](#ëª¨ë¸-ì„±ëŠ¥)
6. [ì£¼ìš” ë°œê²¬ì‚¬í•­](#ì£¼ìš”-ë°œê²¬ì‚¬í•­)
7. [ìƒì—…ìš© ì˜ˆì¸¡ API](#ìƒì—…ìš©-ì˜ˆì¸¡-api)
8. [ì‚¬ìš© ë°©ë²•](#ì‚¬ìš©-ë°©ë²•)

---

## í”„ë¡œì íŠ¸ ê°œìš”

### ëª©ì 
ì‚¬ìš©ìì˜ ì¿¼ë¦¬ì— ëŒ€í•œ **Maximum Acceptable Latency (MAL)**ë¥¼ ì˜ˆì¸¡í•˜ì—¬, ë‹¤ì–‘í•œ ì‚¬ìš©ì retention levelì—ì„œ ìˆ˜ìš© ê°€ëŠ¥í•œ ëŒ€ê¸° ì‹œê°„ì„ ì¶”ì •í•©ë‹ˆë‹¤.

### í•µì‹¬ ì§ˆë¬¸
- **"ì´ ì¿¼ë¦¬ì—ì„œ 90%ì˜ ì‚¬ìš©ìë¥¼ ìœ ì§€í•˜ë ¤ë©´ ìµœëŒ€ ëª‡ ì´ˆ ì•ˆì— ì‘ë‹µí•´ì•¼ í•˜ëŠ”ê°€?"**
- **"ê°œì¸ì°¨ê°€ MAL ì˜ˆì¸¡ì— ì–¼ë§ˆë‚˜ ì¤‘ìš”í•œê°€?"**
- **"ì¿¼ë¦¬ íŠ¹ì„±ë§Œìœ¼ë¡œ ë²”ìš©ì ì¸ MAL ì˜ˆì¸¡ì´ ê°€ëŠ¥í•œê°€?"**

### ì…ë ¥ ë° ì¶œë ¥

**Input:**
- `query`: ì‚¬ìš©ì ì¿¼ë¦¬ í…ìŠ¤íŠ¸ (í•œêµ­ì–´)

**Output:**
- `mal_predictions`: ë‹¤ì–‘í•œ accommodation level (50%, 90%, 95%)ì—ì„œì˜ ì˜ˆì¸¡ MAL (ì´ˆ)
- `mean_mal`: í‰ê·  ì˜ˆì¸¡ MAL
- `extracted_features`: LLMì´ ì¶”ì¶œí•œ 11ê°œ features

---

## ë””ë ‰í† ë¦¬ êµ¬ì¡°

```
LMM_model/
â”‚
â”œâ”€â”€ data/                          # ì›ë³¸ ë° ì²˜ë¦¬ëœ ë°ì´í„°
â”‚   â”œâ”€â”€ all_data.xlsx              # ì›ë³¸ ë°ì´í„° (2560 rows: 256 queries Ã— 80 participants)
â”‚   â”œâ”€â”€ augmented_data.csv         # ì¿¼ë¦¬ë³„ feature ë°ì´í„° (256 queries Ã— 47 features)
â”‚   â””â”€â”€ final_dataset.csv          # ìµœì¢… ê²°í•© ë°ì´í„° (2560 rows Ã— 50 cols)
â”‚
â”œâ”€â”€ models/                        # í•™ìŠµëœ ëª¨ë¸
â”‚   â”œâ”€â”€ lmm_model1.pkl             # Model 1: Participant RE (47 features)
â”‚   â”œâ”€â”€ lmm_model2.pkl             # Model 2: Query RE
â”‚   â””â”€â”€ lmm_model1_selected.pkl    # Feature selection ë²„ì „ (11 features) â­
â”‚
â”œâ”€â”€ scripts/                       # ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸
â”‚   â”œâ”€â”€ join_data.py               # ë°ì´í„° ê²°í•© ìŠ¤í¬ë¦½íŠ¸
â”‚   â”œâ”€â”€ lmm_analysis.py            # LMM ëª¨ë¸ í•™ìŠµ (47 features)
â”‚   â”œâ”€â”€ retrain_model1_selected.py # Feature selection ëª¨ë¸ í•™ìŠµ (11 features)
â”‚   â”œâ”€â”€ commercial_predictor_llm.py # ìƒì—…ìš© ì˜ˆì¸¡ API â­
â”‚   â”œâ”€â”€ feature_selection_analysis.py  # Feature ì„ íƒ ë¶„ì„
â”‚   â”œâ”€â”€ feature_extractor.py       # LLM feature ì¶”ì¶œê¸°
â”‚   â””â”€â”€ model_performance_analysis.py  # ëª¨ë¸ ì„±ëŠ¥ í‰ê°€
â”‚
â”œâ”€â”€ outputs/                       # ê²°ê³¼ ë° ì‹œê°í™”
â”‚   â”œâ”€â”€ lmm_diagnostics.png        # ëª¨ë¸ ì§„ë‹¨ í”Œë¡¯
â”‚   â”œâ”€â”€ random_effects.png         # Random effects ë¶„í¬
â”‚   â”œâ”€â”€ lmm_model1_coefficients.csv           # 47 features ê³„ìˆ˜
â”‚   â”œâ”€â”€ lmm_model1_selected_coefficients.csv  # 11 features ê³„ìˆ˜
â”‚   â”œâ”€â”€ feature_importance.png     # Feature ì¤‘ìš”ë„ ì‹œê°í™”
â”‚   â”œâ”€â”€ vif_analysis.csv           # ë‹¤ì¤‘ê³µì„ ì„± ë¶„ì„
â”‚   â””â”€â”€ lmm_analysis.log           # ì „ì²´ ë¶„ì„ ë¡œê·¸
â”‚
â””â”€â”€ README.md                      # ë©”ì¸ ë¬¸ì„œ (ì´ íŒŒì¼)
```

---

## ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸

### 1ë‹¨ê³„: ë°ì´í„° ê²°í•© (`scripts/join_data.py`)

**ì…ë ¥:**
- `all_data.xlsx`: 80ëª… participants Ã— 256 queries = 2,560 rows
  - Columns: `participant`, `queries`, `MAL`
- `augmented_data.csv`: 256 unique queries Ã— 47 features
  - Columns: `queries`, `MAL`, 47 feature columns

**ì²˜ë¦¬:**
1. `augmented_data.csv`ì˜ ì¿¼ë¦¬ ìˆœì„œë¥¼ ê¸°ì¤€ìœ¼ë¡œ `query_id` ë¶€ì—¬ (1~256)
2. `participant` â†’ `participant_id`ë¡œ rename
3. `queries`ë¥¼ í‚¤ë¡œ ë‘ í…Œì´ë¸” LEFT JOIN
4. `all_data.xlsx`ì˜ MAL ê°’ ì‚¬ìš© (ê°œì¸ë³„ë¡œ ë‹¤ë¦„)
5. `augmented_data.csv`ì˜ features ì‚¬ìš© (ì¿¼ë¦¬ë³„ë¡œ ë™ì¼)

**ì¶œë ¥:**
- `final_dataset.csv`: 2,560 rows Ã— 50 columns
  - `participant_id`, `query_id`, `MAL`, 47 features

**ê²€ì¦:**
- âœ… 2,560 rows (256 queries Ã— 80 participants)
- âœ… Query ID: 1~256
- âœ… MAL ê°’: all_data ê¸°ì¤€ (ê°œì¸ë³„ ì°¨ì´ ë°˜ì˜)
- âœ… Features: augmented_data ê¸°ì¤€ (ì¿¼ë¦¬ë³„ ë™ì¼)
- âœ… ê²°ì¸¡ê°’ ì—†ìŒ

### 2ë‹¨ê³„: ë¡œê·¸ ë³€í™˜

**ì´ìœ :**
- ì›ë³¸ MAL ë¶„í¬: ì˜¤ë¥¸ìª½ìœ¼ë¡œ ê¸´ ê¼¬ë¦¬ (right-skewed)
- ì„ í˜• í˜¼í•© ëª¨ë¸ì€ ì •ê·œì„± ê°€ì • í•„ìš”

**ë³€í™˜:**
```python
log_MAL = log(MAL)
```

**ê²°ê³¼:**
- MAL ë²”ìœ„: [0.61, 211.28] ì´ˆ
- log(MAL) ë²”ìœ„: [-0.50, 5.35]
- ì •ê·œë¶„í¬ì— ê·¼ì ‘

---

## ëª¨ë¸ë§ ë°©ë²•ë¡ 

### ìˆ˜í•™ì  ëª¨ë¸

```
log(MAL_ij) = Î²â‚€ + Î²'X_i + u_j + Îµ_ij
```

**êµ¬ì„± ìš”ì†Œ:**
- `i`: query index (1~256)
- `j`: participant index (80ëª…)
- `Î²â‚€`: Intercept (ì „ì²´ í‰ê· )
- `Î²'X_i`: Fixed effects (ì¿¼ë¦¬ featuresì˜ íš¨ê³¼)
- `u_j`: Participant random effect (ê°œì¸ ê³ ìœ  ì„±í–¥)
  - `u_j ~ N(0, Ïƒ_uÂ²)`
- `Îµ_ij`: Residual error
  - `Îµ_ij ~ N(0, Ïƒ_ÎµÂ²)`

### ë‘ ê°€ì§€ ëª¨ë¸ ë¹„êµ

#### Model 1: Participant Random Effect (ì±„íƒ âœ…)
```
log(MAL_ij) = Î²â‚€ + Î²'X_i + u_participant(j) + Îµ_ij
```
- **Grouping variable**: participant_id
- **ê°€ì •**: ì‚¬ëŒë§ˆë‹¤ ê¸°ë³¸ MAL ì„±í–¥ì´ ë‹¤ë¦„
- **Features**: 47ê°œ (ì „ì²´) ë˜ëŠ” 11ê°œ (ì„ íƒ)

#### Model 2: Query Random Effect (ë¯¸ì±„íƒ âŒ)
```
log(MAL_ij) = Î²â‚€ + Î²'X_i + v_query(i) + Îµ_ij
```
- **Grouping variable**: query_id
- **ê°€ì •**: ì¿¼ë¦¬ë§ˆë‹¤ featureë¡œ ì„¤ëª… ì•ˆ ë˜ëŠ” ì¶”ê°€ íš¨ê³¼

### ëª¨ë¸ ì„ íƒ ì´ìœ 

| Metric | Model 1 (Participant RE) | Model 2 (Query RE) | ìŠ¹ì |
|--------|--------------------------|-------------------|------|
| Log-Likelihood | -1,734.50 | -3,152.75 | âœ… Model 1 |
| ICC | **0.7208** | 0.0104 | âœ… Model 1 |
| Random Variance | 0.4697 | 0.0067 | âœ… Model 1 |

**ê²°ë¡ :** Model 1ì´ ì••ë„ì ìœ¼ë¡œ ìš°ìˆ˜ â†’ **ê°œì¸ì°¨ê°€ ì¿¼ë¦¬ ì°¨ì´ë³´ë‹¤ í›¨ì”¬ ì¤‘ìš”**

---

## ëª¨ë¸ ì„±ëŠ¥

### í†µê³„ì  ì„±ëŠ¥ ì§€í‘œ ğŸ“Š

**Selected Model (11 features, VIF < 10):**

| Metric | Value | í•´ì„ |
|--------|-------|------|
| **RÂ² (log scale)** | **0.7361** | ëª¨ë¸ì´ log(MAL) ë¶„ì‚°ì˜ 73.6%ë¥¼ ì„¤ëª… |
| **Pearson r (log scale)** | 0.8580 | ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ì˜ ê°•í•œ ìƒê´€ê´€ê³„ |
| **RMSE (log scale)** | 0.4299 | ë¡œê·¸ ì²™ë„ì—ì„œ í‰ê·  ì˜ˆì¸¡ ì˜¤ì°¨ |
| **RMSE (original scale)** | 1.54ì´ˆ | ì›ë˜ ì²™ë„(ì´ˆ)ì—ì„œ í‰ê·  ì˜ˆì¸¡ ì˜¤ì°¨ |
| **Log-Likelihood** | -1,720.35 | ëª¨ë¸ ì í•©ë„ (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ) |
| **ICC** | 0.7092 | ì „ì²´ ë¶„ì‚° ì¤‘ ê°œì¸ì°¨ê°€ ì°¨ì§€í•˜ëŠ” ë¹„ìœ¨ |

**ë°ì´í„° ìš”ì•½ (Original Scale):**
- Mean MAL: 21.89ì´ˆ
- Median MAL: 15.87ì´ˆ
- Std MAL: 20.26ì´ˆ
- Range: [0.61, 211.28]ì´ˆ

### ë¶„ì‚° ë¶„í•´ - í•µì‹¬ ë°œê²¬ ğŸ¯

**Selected Model (11 features):**
```
Participant variance (u): 0.4671  â†’  70.9% â­â­â­
Residual variance (Îµ):    0.1915  â†’  29.1%
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total variance:           0.6586     100%
```

**ICC (Intraclass Correlation) = 0.7092**

**í•´ì„:**
- ê°™ì€ ì‚¬ëŒì´ ë‹¤ë¥¸ ì¿¼ë¦¬ì— ë‹µí•  ë•Œ, MALì˜ **71%ëŠ” ê·¸ ì‚¬ëŒ ê³ ìœ ì˜ ì„±í–¥**ìœ¼ë¡œ ì„¤ëª…ë¨
- **"ê¸°ë‹¤ë¦¼ tolerance"ëŠ” ê°œì¸ íŠ¹ì„±**ì´ë©°, ì¿¼ë¦¬ ë‚´ìš©ë³´ë‹¤ í›¨ì”¬ ì¤‘ìš”!
- ê°œì¸í™” ì˜ˆì¸¡ì´ ì´ìƒì ì´ì§€ë§Œ, ë²”ìš© ì˜ˆì¸¡ë„ ì˜ë¯¸ ìˆìŒ
- **RÂ² = 0.74**: ëª¨ë¸ì´ ë°ì´í„°ë¥¼ ë§¤ìš° ì˜ ì„¤ëª…í•¨ (ì‚¬íšŒê³¼í•™ ê¸°ì¤€ ìš°ìˆ˜)

### Feature Selection: 47ê°œ â†’ 11ê°œ

**ìµœì¢… ì„ íƒëœ 11ê°œ Features (p-value ê¸°ì¤€ ì •ë ¬):**

| Feature | ê³„ìˆ˜ | p-value | í•´ì„ |
|---------|------|---------|------|
| **expected_answer_length** | +0.224 | 4.33e-43 â­â­â­ | ê¸´ ë‹µë³€ ê¸°ëŒ€ ì‹œ MAL ì¦ê°€ |
| **output_requires_multimedia_creation** | +0.657 | 3.66e-17 â­â­â­ | ë©€í‹°ë¯¸ë””ì–´ ìƒì„± í•„ìš” ì‹œ MAL 1.9ë°° ì¦ê°€ |
| **has_comparative_phrase** | +0.275 | 2.45e-15 â­â­â­ | ë¹„êµ í‘œí˜„ í¬í•¨ ì‹œ MAL 1.3ë°° ì¦ê°€ |
| **novelty_seeking** | -0.165 | 2.18e-08 â­â­ | ìƒˆë¡œìš´ ì •ë³´ íƒìƒ‰ ì‹œ MAL ê°ì†Œ |
| **needs_health_data** | -0.137 | 2.00e-06 â­â­ | ê±´ê°• ë°ì´í„° ê´€ë ¨ ì¿¼ë¦¬ëŠ” ë¹ ë¥¸ ì‘ë‹µ ê¸°ëŒ€ |
| **planning_horizon** | +0.063 | 1.75e-05 â­â­ | ì¥ê¸° ê³„íš ì¿¼ë¦¬ ì‹œ MAL ì¦ê°€ |
| **time_urgency_level** | -0.051 | 0.0001 â­ | ê¸´ê¸‰ë„ ë†’ì„ìˆ˜ë¡ MAL ê°ì†Œ |
| **device_context_implied** | -0.049 | 0.0022 â­ | ëª¨ë°”ì¼ ë§¥ë½ ì‹œ MAL ê°ì†Œ |
| **time_window_length** | +0.040 | 0.012 â­ | ê¸´ ì‹œê°„ ë²”ìœ„ ì¿¼ë¦¬ ì‹œ MAL ì¦ê°€ |
| requires_aggregation | +0.042 | 0.079 | í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•˜ì§€ ì•ŠìŒ |
| social_context_strength | +0.007 | 0.589 | í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•˜ì§€ ì•ŠìŒ |

**í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•œ features (p < 0.05): 9ê°œ**

### ëª¨ë¸ ì§„ë‹¨

![ëª¨ë¸ ì§„ë‹¨](outputs/lmm_diagnostics.png)

**ì§„ë‹¨ í”Œë¡¯ í•´ì„:**
- **Model 1 (ìƒë‹¨)**: Participant Random Effect
  - Residuals vs Fitted: íŒ¨í„´ ì—†ìŒ (ì¢‹ìŒ)
  - Q-Q Plot: ì •ê·œë¶„í¬ì— ê·¼ì ‘
  - Residual Distribution: ëŒ€ì¹­ì 
- **Model 2 (í•˜ë‹¨)**: Query Random Effect
  - Residual ë¶„ì‚°ì´ ë” í¬ê³  íŒ¨í„´ ì¡´ì¬

![Random Effects](outputs/random_effects.png)

**Random Effects ë¶„í¬:**
- **Participant Random Effects (ì¢Œì¸¡)**: ë„“ì€ ë¶„í¬ (SD â‰ˆ 0.69)
  - ê°œì¸ì°¨ê°€ ë§¤ìš° í¼ì„ ë³´ì—¬ì¤Œ
- **Query Random Effects (ìš°ì¸¡)**: ì¢ì€ ë¶„í¬ (SD â‰ˆ 0.08)
  - ì¿¼ë¦¬ íš¨ê³¼ëŠ” ë¯¸ë¯¸í•¨

---

## ì£¼ìš” ë°œê²¬ì‚¬í•­

### 1. ê°œì¸ì°¨ê°€ ì••ë„ì ìœ¼ë¡œ ì¤‘ìš” ğŸ¯

```
ì „ì²´ ë¶„ì‚°ì˜ 72% = ê°œì¸ ê°„ ì°¨ì´
ì „ì²´ ë¶„ì‚°ì˜ 28% = ê°œì¸ ë‚´ ë³€ë™ + ì¿¼ë¦¬ íš¨ê³¼
```

**ì˜ë¯¸:**
- MALì€ **ê°œì¸ íŠ¹ì„±**ì´ ì£¼ëœ ê²°ì • ìš”ì¸
- "ì´ ì¿¼ë¦¬ëŠ” Xì´ˆë‹¤"ê°€ ì•„ë‹ˆë¼ **"ì´ ì‚¬ëŒì—ê²Œ ì´ ì¿¼ë¦¬ëŠ” Xì´ˆë‹¤"**
- í•˜ì§€ë§Œ ê°œì¸ ì •ë³´ ì—†ì´ë„ ì¿¼ë¦¬ íŠ¹ì„±ìœ¼ë¡œ ë²”ìš© ì˜ˆì¸¡ ê°€ëŠ¥

### 2. Feature Selectionìœ¼ë¡œ íš¨ìœ¨ì„± í–¥ìƒ ğŸ“Š

**47ê°œ â†’ 11ê°œ features**
- LLM API í† í° ë¹„ìš© 77% ê°ì†Œ
- Feature ì¶”ì¶œ ì†ë„ í–¥ìƒ
- ë‹¤ì¤‘ê³µì„ ì„± ì œê±°
- ëª¨ë¸ í•´ì„ë ¥ í–¥ìƒ

**9ê°œ significant features (p < 0.05):**
- ë‹µë³€ ê¸¸ì´, ë©€í‹°ë¯¸ë””ì–´ ìƒì„±, ë¹„êµ í‘œí˜„ì´ ê°€ì¥ ê°•í•œ ì˜í–¥
- ê¸´ê¸‰ë„, ê±´ê°• ê´€ë ¨ ì¿¼ë¦¬ëŠ” ë¹ ë¥¸ ì‘ë‹µ ê¸°ëŒ€
- ê³„íš ë²”ìœ„, ì‹œê°„ ë²”ìœ„ê°€ ê¸´ ì¿¼ë¦¬ëŠ” ë†’ì€ MAL ìˆ˜ìš©

### 3. Population-level ì˜ˆì¸¡ ì „ëµ

**ìƒì—…ìš© ì˜ˆì¸¡ê¸° íŠ¹ì§•:**
- Participant ID ë¶ˆí•„ìš” (ë²”ìš© ì˜ˆì¸¡)
- Fixed effects + **population mean random effect** ì‚¬ìš©
- ê°œì¸ë³„ random effect ëŒ€ì‹  ì „ì²´ í‰ê·  ì‚¬ìš©
- ì •í™•ë„ëŠ” ë‚®ì•„ì§€ì§€ë§Œ ë²”ìš©ì„± í™•ë³´

---

## ìƒì—…ìš© ì˜ˆì¸¡ API

### CommercialMALPredictorLLM

**ìœ„ì¹˜:** `scripts/commercial_predictor_llm.py`

**íŠ¹ì§•:**
- âœ… **Participant ID ë¶ˆí•„ìš”** (Population-level prediction)
- âœ… **LLM ê¸°ë°˜ ìë™ feature ì¶”ì¶œ** (GPT-4o-mini or Claude)
- âœ… **11ê°œ featuresë§Œ ì¶”ì¶œ** (47ê°œ ëŒ€ë¹„ 77% ë¹„ìš© ì ˆê°)
- âœ… **ë‹¤ì–‘í•œ accommodation level ì§€ì›** (50%, 90%, 95% ë“±)
- âœ… **Percentile ê¸°ë°˜ ì˜ˆì¸¡** (ì‚¬ìš©ì retention ìµœì í™”)

### ì‚¬ìš© ì˜ˆì‹œ

```python
from commercial_predictor_llm import CommercialMALPredictorLLM

# ì˜ˆì¸¡ê¸° ì´ˆê¸°í™”
predictor = CommercialMALPredictorLLM(
    api_key="your-openai-api-key",  # ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ OPENAI_API_KEY
)

# ë‹¨ì¼ ì˜ˆì¸¡
query = "ì§€ë‚œì£¼ì— ì°ì€ ê³¨í”„ ìŠ¤ìœ™ ì˜ìƒ ë³´ì—¬ì¤˜"
result = predictor.predict(
    query=query,
    accommodation_levels=[50, 90, 95]
)

print(result['mal_predictions'])
# Output: {'50%': 8.5, '90%': 18.2, '95%': 23.7}

print(result['mean_mal'])
# Output: 10.5

print(result['interpretation'])
# Output:
#   â€¢ 95% retention (5% churn): 23.7s (keep 95% of users)
#   â€¢ 90% retention (10% churn): 18.2s (keep 90% of users)
#   â€¢ 50% retention (50% churn): 8.5s (keep 50% of users)
#
#   Recommendation:
#     â†’ Design for 18.2s to keep 90% of users (10% churn)
```

### API ì‘ë‹µ êµ¬ì¡°

```python
{
    'query': 'ì§€ë‚œì£¼ì— ì°ì€ ê³¨í”„ ìŠ¤ìœ™ ì˜ìƒ ë³´ì—¬ì¤˜',
    'features': {
        'needs_health_data': 0,
        'expected_answer_length': 0,
        'planning_horizon': 0,
        'time_window_length': 1,
        'time_urgency_level': 0,
        'novelty_seeking': 0,
        'requires_aggregation': 0,
        'has_comparative_phrase': 0,
        'device_context_implied': 0,
        'output_requires_multimedia_creation': 0,
        'social_context_strength': 0
    },
    'mal_predictions': {
        '50%': 8.5,
        '90%': 18.2,
        '95%': 23.7
    },
    'mean_mal': 10.5,
    'log_mal_mean': 2.35,
    'total_std': 0.68,
    'interpretation': '...'
}
```

### ì˜ˆì¸¡ ë°©ì‹

```python
# 1. LLMì´ 11ê°œ features ìë™ ì¶”ì¶œ
features = llm_extract(query)

# 2. Fixed effects ê³„ì‚°
log_mal_mean = intercept + sum(Î²_i * feature_i)

# 3. Population mean random effect ì¶”ê°€
log_mal_mean += population_mean_random_effect

# 4. Percentile ê¸°ë°˜ MAL ê³„ì‚°
for level in [50, 90, 95]:
    churn_rate = (100 - level) / 100
    z_score = norm.ppf(churn_rate)
    log_mal_p = log_mal_mean + z_score * total_std
    mal_sec = exp(log_mal_p)
```

**ì¤‘ìš”:**
- 90% retention = 10% churn â†’ 10th percentile MAL
- ë†’ì€ retentionì„ ì›í• ìˆ˜ë¡ ë” ì§§ì€ MAL í•„ìš”

### 11ê°œ Features ì •ì˜

1. **needs_health_data** (BINARY: 0 or 1)
   - ê±´ê°•/ìš´ë™ ë°ì´í„° í•„ìš” ì—¬ë¶€

2. **expected_answer_length** (ORDINAL: 0-2)
   - 0 = ë‹¨ì¼ ì•„ì´í…œ, 1 = ë¦¬ìŠ¤íŠ¸, 2 = ê¸´ ë¬¸ì„œ

3. **planning_horizon** (ORDINAL: 0-3)
   - 0 = ì¡°íšŒ, 1 = ë‹¨ê¸°, 2 = ì¤‘ê¸°, 3 = ì¥ê¸°

4. **time_window_length** (ORDINAL: 0-3)
   - 0 = íŠ¹ì • ì‹œì , 1 = ë©°ì¹ , 2 = ëª‡ ì£¼, 3 = ëª‡ ë‹¬+

5. **time_urgency_level** (ORDINAL: 0-2)
   - 0 = ê¸´ê¸‰í•˜ì§€ ì•ŠìŒ, 1 = ë³´í†µ, 2 = ë§¤ìš° ê¸´ê¸‰

6. **novelty_seeking** (BINARY: 0 or 1)
   - ìƒˆë¡œìš´ ì •ë³´ íƒìƒ‰ ì—¬ë¶€

7. **requires_aggregation** (BINARY: 0 or 1)
   - ì§‘ê³„/ê³„ì‚° í•„ìš” ì—¬ë¶€

8. **has_comparative_phrase** (BINARY: 0 or 1)
   - ë¹„êµ í‘œí˜„ í¬í•¨ ì—¬ë¶€

9. **device_context_implied** (ORDINAL: 0-2)
   - 0 = ê¸°ê¸° ë¬´ê´€, 1 = ëª¨ë°”ì¼, 2 = ë°ìŠ¤í¬í†±

10. **output_requires_multimedia_creation** (BINARY: 0 or 1)
    - ë©€í‹°ë¯¸ë””ì–´ ìƒì„± í•„ìš” ì—¬ë¶€

11. **social_context_strength** (ORDINAL: 0-2)
    - ì‚¬íšŒì  ë§¥ë½ ê°•ë„

---

## ì‚¬ìš© ë°©ë²•

### ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰

```bash
cd LMM_model

# 1. ë°ì´í„° ê²°í•©
python scripts/join_data.py

# 2. LMM ëª¨ë¸ í•™ìŠµ (47 features)
python scripts/lmm_analysis.py

# 3. Feature selection ëª¨ë¸ í•™ìŠµ (11 features)
python scripts/retrain_model1_selected.py

# 4. Feature ì„ íƒ ë¶„ì„ (ì„ íƒì‚¬í•­)
python scripts/feature_selection_analysis.py
```

### ìƒì—…ìš© ì˜ˆì¸¡ê¸° ì‚¬ìš©

```bash
# í™˜ê²½ë³€ìˆ˜ ì„¤ì •
export OPENAI_API_KEY="your-api-key"

# Pythonì—ì„œ ì‚¬ìš©
python
>>> from scripts.commercial_predictor_llm import CommercialMALPredictorLLM
>>> predictor = CommercialMALPredictorLLM()
>>> result = predictor.predict("ì§€ë‚œì£¼ì— ì°ì€ ê³¨í”„ ìŠ¤ìœ™ ì˜ìƒ ë³´ì—¬ì¤˜", [50, 90, 95])
>>> print(result['mal_predictions'])
```

### Batch ì˜ˆì¸¡

```python
queries = [
    "ì§€ë‚œì£¼ì— ì°ì€ ê³¨í”„ ìŠ¤ìœ™ ì˜ìƒ ë³´ì—¬ì¤˜",
    "ì˜¤ëŠ˜ ë‚ ì”¨ ì–´ë•Œ?",
    "ì´ë²ˆ ë‹¬ ì»¤í”¼ ì§€ì¶œ ì–¼ë§ˆì•¼?"
]

results_df = predictor.batch_predict(queries, [50, 90, 95])
print(results_df)
```

---

## í™˜ê²½ ì„¤ì •

### í•„ìˆ˜ íŒ¨í‚¤ì§€

```bash
pip install pandas numpy scipy statsmodels matplotlib seaborn scikit-learn openpyxl openai
```

### Python ë²„ì „
- Python 3.9+

---

## íŒŒì¼ ì„¤ëª…

### ë°ì´í„° íŒŒì¼
- `data/all_data.xlsx`: ì›ë³¸ MAL ë°ì´í„° (2560 rows)
- `data/augmented_data.csv`: ì¿¼ë¦¬ë³„ features (256 rows Ã— 47 features)
- `data/final_dataset.csv`: ê²°í•©ëœ ìµœì¢… ë°ì´í„°

### ëª¨ë¸ íŒŒì¼
- `models/lmm_model1.pkl`: 47 features ëª¨ë¸
- `models/lmm_model1_selected.pkl`: 11 features ëª¨ë¸ (ì¶”ì²œ â­)
- `models/lmm_model2.pkl`: Query random effect ëª¨ë¸

### ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼
- `scripts/join_data.py`: ë°ì´í„° ì „ì²˜ë¦¬
- `scripts/lmm_analysis.py`: 47 features ëª¨ë¸ í•™ìŠµ
- `scripts/retrain_model1_selected.py`: 11 features ëª¨ë¸ í•™ìŠµ
- `scripts/commercial_predictor_llm.py`: ìƒì—…ìš© ì˜ˆì¸¡ API (ìµœì¢… ë²„ì „ â­)
- `scripts/feature_selection_analysis.py`: Feature ì¤‘ìš”ë„ ë¶„ì„
- `scripts/feature_extractor.py`: LLM feature ì¶”ì¶œê¸°

### ì¶œë ¥ íŒŒì¼
- `outputs/lmm_diagnostics.png`: ëª¨ë¸ ì§„ë‹¨ í”Œë¡¯
- `outputs/random_effects.png`: Random effects ë¶„í¬
- `outputs/lmm_model1_coefficients.csv`: 47 features ê³„ìˆ˜
- `outputs/lmm_model1_selected_coefficients.csv`: 11 features ê³„ìˆ˜
- `outputs/lmm_analysis.log`: ì „ì²´ ë¶„ì„ ë¡œê·¸

---

## ğŸ“Š ëª¨ë¸ ìš”ì•½ ì¹´ë“œ

| Metric | Value |
|--------|-------|
| **Training Data** | 2,560 observations (256 queries Ã— 80 participants) |
| **Features (Full Model)** | 47 |
| **Features (Selected Model)** | 11 (9 significant at p < 0.05) |
| **Model Type** | Linear Mixed Model with Participant Random Effect |
| **RÂ² (log scale)** | **0.7361** (73.6% variance explained) |
| **RMSE (original scale)** | 1.54 seconds |
| **Pearson correlation** | 0.8580 |
| **Log-Likelihood** | -1,720.35 |
| **Participant Variance** | 0.4671 (70.9% of total) |
| **Residual Variance** | 0.1915 (29.1% of total) |
| **ICC** | 0.7092 (71% variance from individual differences) |
| **Deployment** | Population-level prediction (no participant ID needed) |
| **Feature Extraction** | LLM-based (GPT-4o-mini) |
| **Cost Reduction** | 77% (47 â†’ 11 features) |

---

## âœ¨ TL;DR

ì´ í”„ë¡œì íŠ¸ëŠ” **LLM ê¸°ë°˜ ë²”ìš© MAL ì˜ˆì¸¡**ì„ ì œê³µí•©ë‹ˆë‹¤:

**í•µì‹¬ ë°œê²¬:**
- âœ… **RÂ² = 0.74**: ëª¨ë¸ì´ MAL ë¶„ì‚°ì˜ 73.6%ë¥¼ ì„¤ëª… (ìš°ìˆ˜í•œ ì˜ˆì¸¡ë ¥)
- âœ… 71%ì˜ ë¶„ì‚°ì„ ê°œì¸ì°¨ë¡œ ì„¤ëª… (ICC = 0.71)
- âœ… Feature selection: 47ê°œ â†’ 11ê°œ (77% ë¹„ìš© ì ˆê°)
- âœ… 9ê°œ í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•œ features (p < 0.05)
- âœ… Population-level ì˜ˆì¸¡ìœ¼ë¡œ ë²”ìš©ì„± í™•ë³´

**ìƒì—…ìš© API:**
```python
predictor = CommercialMALPredictorLLM()
result = predictor.predict("ì§€ë‚œì£¼ì— ì°ì€ ê³¨í”„ ìŠ¤ìœ™ ì˜ìƒ ë³´ì—¬ì¤˜", [50, 90, 95])
print(result['mal_predictions'])
# {'50%': 8.5, '90%': 18.2, '95%': 23.7}
```

**í•µì‹¬ ì¸ì‚¬ì´íŠ¸**:
- "ê¸°ë‹¤ë¦¼ toleranceëŠ” ê°œì¸ íŠ¹ì„±ì´ì§€ë§Œ, ì¿¼ë¦¬ íŠ¹ì„±ë§Œìœ¼ë¡œë„ ë²”ìš© ì˜ˆì¸¡ ê°€ëŠ¥"
- "11ê°œ featuresë§Œìœ¼ë¡œ íš¨ìœ¨ì ì¸ ì˜ˆì¸¡ ê°€ëŠ¥ (ë¹„ìš© 77% ì ˆê°)"
- "90% retentionì„ ì›í•œë‹¤ë©´ ì˜ˆì¸¡ëœ 90% MAL ì´ë‚´ë¡œ ì‘ë‹µ í•„ìš”"

**Accommodation Level í•´ì„:**
- 50% retention (50% churn): ì¤‘ê°„ MAL (ì ˆë°˜ì˜ ì‚¬ìš©ì ìœ ì§€)
- 90% retention (10% churn): ì§§ì€ MAL (90% ì‚¬ìš©ì ìœ ì§€)
- 95% retention (5% churn): ë§¤ìš° ì§§ì€ MAL (95% ì‚¬ìš©ì ìœ ì§€)

---

**End of Documentation**
